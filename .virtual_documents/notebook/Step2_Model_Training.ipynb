





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


# for machine learning 
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression, Ridge,Lasso
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import RandomizedSearchCV
# from catboost import CatBoostRegressor
from xgboost import XGBRegressor
import warnings
warnings.filterwarnings('ignore')


import os
os.chdir("D:/All_the_Documents/Machine Learning Content/My_programs_VS_code/KN_machine_learning_projects")
df = pd.read_csv("notebook/data/stud.csv")


df.head()





X = df.drop(columns=['math_score'], axis= 1)
y = df['math_score']



X.head()


# getting the categorical and num features here
num_features = X.select_dtypes(exclude='object').columns
cat_features = X.select_dtypes(include='object').columns


num_features


cat_features


from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

numeric_transformer = StandardScaler()
oh_transformer = OneHotEncoder()


preprocessor = ColumnTransformer(
    [
        ('StandardScalar', numeric_transformer, num_features),
        ('OneHotEncoder', oh_transformer, cat_features),
    ]
)


X = preprocessor.fit_transform(X)


X.shape





from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=42 )


X_train





def evaluate_model(true, predicted):
    mae = mean_absolute_error(true, predicted)
    mse = mean_squared_error(true, predicted)
    rmse = np.sqrt(mse)
    r2 = r2_score(true, predicted)

    return (mae, rmse, r2)


import warnings
warnings.filterwarnings("ignore", category=UserWarning)


models = {
    "Linear Regression": LinearRegression(),
    "Lasso": Lasso(),
    "Ridge": Ridge(),
    "K-Neighbors Regressor": KNeighborsRegressor(),
    "Decision Tree": DecisionTreeRegressor(),
    "Random Forest Regressor": RandomForestRegressor(),
    "XGBRegressor": XGBRegressor(), 
    # "CatBoosting Regressor": CatBoostRegressor(verbose=False),
    "AdaBoost Regressor": AdaBoostRegressor()
}

model_list = []
r2_score_list = []

for i in range(len(list(models.keys()))):
    model = list(models.values())[i]
    model.fit(X_train, y_train)

    y_predict_train = model.predict(X_train)
    y_predict_test = model.predict(X_test)

    model_train_mae , model_train_rmse, model_train_r2 = evaluate_model(y_train, y_predict_train)
    model_test_mae , model_test_rmse, model_test_r2 = evaluate_model(y_test, y_predict_test)


    model_list.append(list(models.keys())[i])
    r2_score_list.append(model_test_r2)
    

    


pd.DataFrame(list(zip(model_list, r2_score_list)), columns=['Model', 'R2_Score']).sort_values(by=['R2_Score'], ascending= False)





lin_model = LinearRegression()
lin_model.fit(X_train, y_train)

 
y_predict = lin_model.predict(X_test)

print(f'The Accuracy of the Linear Model is : {r2_score(y_test, y_predict)* 100}')


plt.scatter(y_test, y_predict)
plt.xlabel('Actual');
plt.ylabel('Predicted')
plt.grid()
plt.show()


sns.regplot(x = y_test, y = y_predict, color = 'red')
plt.show()





pd.DataFrame({"Actual Value": y_test, "Predicted Value": y_predict, "Difference": y_test - y_predict})


# random_cv = RandomizedSearchCV(estimator= lin_modelm, cv= 5, scoring= 'accuracy')
# here we don't use the huperparameter tuning, it is used in the form ridge, Lasso and Elastic
# that's why we will avoid it here.









